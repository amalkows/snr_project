{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przygotowanie środowiska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adama\\AppData\\Local\\conda\\conda\\envs\\stable\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import keras\n",
    "import functools \n",
    "import datetime\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn import svm\n",
    "\n",
    "from skimage import exposure, feature\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.applications.vgg19 import VGG19, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.optimizers import SGD, Adam\n",
    "from scipy.ndimage.interpolation import rotate, shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k(n):\n",
    "    top_k_acc = functools.partial(keras.metrics.top_k_categorical_accuracy, k=n)\n",
    "    top_k_acc.__name__ = 'top_'+str(n)\n",
    "    return top_k_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmc(model, test_set, d_3, SVM):    \n",
    "    if SVM is not None:\n",
    "        with tf.Session() as sess:\n",
    "            cmc_curves = [0]\n",
    "            for i in range(1, class_count+1):\n",
    "                cmc_curves.append(\n",
    "                    top_k(i)(\n",
    "                        np_utils.to_categorical(test_set[:, 1], num_classes=class_count), \n",
    "                        SVM.astype(np.float32)\n",
    "                    ).eval()\n",
    "                )\n",
    "    else:\n",
    "        model.compile(optimizer=SGD(lr=0.0005, momentum=0.9),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy']+[top_k(k) for k in range(2, class_count + 1)])\n",
    "        if d_3:\n",
    "            cmc_curves = model.evaluate(np.stack(test_set[:, 0], axis = 0).reshape((test_set.shape[0], size_img, size_img, 3)), \n",
    "                           np_utils.to_categorical(test_set[:, 1], num_classes=class_count),\n",
    "                           batch_size=32,\n",
    "                           verbose = 1)\n",
    "        else:\n",
    "            cmc_curves = model.evaluate(np.stack(test_set[:, 0], axis = 0), \n",
    "                           np_utils.to_categorical(test_set[:, 1], num_classes=class_count),\n",
    "                           batch_size=32,\n",
    "                           verbose = 1) \n",
    "        \n",
    "    cmc_curves = np.array([0] + cmc_curves[1:])*100\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Dokładność Top-k [%]')\n",
    "    plt.plot(range(class_count + 1), np.array(range(class_count + 1))*100/(class_count), color='green', linestyle='--')\n",
    "    plt.plot(range(class_count + 1), cmc_curves)\n",
    "    plt.scatter([1, 5], [cmc_curves[1], cmc_curves[5]])\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    print(\"Dokładność TOP-1:\", '%0.2f' % cmc_curves[1], end=\"%\\n\")\n",
    "    print(\"Dokładność TOP-5:\", '%0.2f' % cmc_curves[5], end=\"%\\n\")\n",
    "    return cmc_curves\n",
    "\n",
    "def more_cmc(cmc_set, names, leg_title, filename):\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Dokładność Top-k [%]')\n",
    "    plt.plot(range(class_count + 1), np.array(range(class_count + 1))*100/(class_count), color='green', linestyle='--')\n",
    "    for curve, name in zip(cmc_set, names):\n",
    "        plt.plot(range(class_count + 1), curve, label=name)\n",
    "        plt.scatter([1, 5], [curve[1], curve[5]])\n",
    "        plt.legend(loc = 'lower right', title = leg_title)\n",
    "    plt.grid()\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename, dpi = 200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def history(hist1, hist2):\n",
    "    plt.xlabel('Numer epoki')\n",
    "    plt.ylabel('Dokładność na zbiorze trenującym [%]')\n",
    "    epochs = len(hist1.epoch) + len(hist2.epoch)\n",
    "    \n",
    "    plt.plot(range(1, epochs+1), np.array(hist1.history['acc'] + hist2.history['acc'])*100, \n",
    "             range(1, epochs+1), np.array(hist1.history['top_5'] + hist2.history['top_5'])*100)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    print(\"Historia uczenia:\")\n",
    "    print(\"\\tEpoka:\\t\", np.array(range(1, epochs+1)))\n",
    "    print(\"\\tTop-1:\\t\", (np.array(hist1.history['acc'] + hist2.history['acc'])*100))\n",
    "    print(\"\\tTop-5:\\t\", (np.array(hist1.history['top_5'] + hist2.history['top_5'])*100))\n",
    "    \n",
    "def more_history(history_set_1, history_set_2, t, names, leg_title, filename):\n",
    "    plt.xlabel('Numer epoki')\n",
    "    plt.ylabel('Dokładność na zbiorze trenującym [%]')\n",
    "    \n",
    "    for hist_1, hist_2, name in zip(history_set_1, history_set_2, names):\n",
    "        epochs = len(hist_1.epoch) + len(hist_2.epoch)\n",
    "        plt.plot(range(1, epochs+1), np.array(hist_1.history[t] + hist_2.history[t])*100, label=name)\n",
    "        plt.legend(loc = 'lower right', title = leg_title)\n",
    "\n",
    "    if len(history_set_1) > len(history_set_2):\n",
    "        for hist, name in zip(history_set_1[len(history_set_2):], names[len(history_set_2):]):\n",
    "            epochs = len(hist.epoch)\n",
    "            plt.plot(range(1, epochs+1), np.array(hist.history[t])*100, label=name)\n",
    "            plt.legend(loc = 'lower right', title = leg_title)\n",
    "\n",
    "    plt.grid()\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename, dpi = 200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skrypt sterujący"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"D:\\\\Studia\\\\18L\\\\SNR\\\\Projekt\\\\\"\n",
    "\n",
    "path = folder_path + \"SET_C\\\\\"\n",
    "bounding_boxes = folder_path + \"bounding_boxes.txt\"\n",
    "\n",
    "class_count = 50\n",
    "\n",
    "size_img = 128\n",
    "std_padding = 2 #USTALONE\n",
    "\n",
    "cnn_ref = None\n",
    "base_cnn_ref = None\n",
    "perceptron = None\n",
    "cnn = None\n",
    "svm_cnn = None\n",
    "\n",
    "add_img = True \n",
    "add_img_count = 1#5   #USTALONE\n",
    "add_img_gauss = True\n",
    "add_img_gauss_std = 5  #USTALONE\n",
    "add_type = 3\n",
    "max_angle = 5   #USTALONE\n",
    "max_shift = 10  #USTALONE\n",
    "\n",
    "covered = 1.0\n",
    "covered_size = 40\n",
    "\n",
    "test_elements = [0, 3, 7, 12, 13, 14, 28, 29, 47, 54] #Numery wybrane losowo - NIEZMIENNE!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przetwarzanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boxes = {}\n",
    "for row in open(bounding_boxes).read().split('\\n'):\n",
    "    if row == \"\":\n",
    "        break\n",
    "    tmp = row.split()\n",
    "    tmp[0] = tmp[0].replace(\"-\", \"\")\n",
    "    boxes[tmp[0]+\".jpg\"] = list(map(int, tmp[1:5]))\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "test_set = []\n",
    "training_set = []\n",
    "data_set_size = 3000.0\n",
    "k=0.0\n",
    "for cls, folder in enumerate(os.listdir(path)):\n",
    "    for index, file in enumerate(os.listdir(path+folder)):\n",
    "        if k % 10:\n",
    "            print('Postęp przetwarzania: %0.1f' % (k*100/data_set_size) + \"%\", end='\\r')\n",
    "        sys.stdout.flush()\n",
    "        k += 1\n",
    "        img = cv2.imread(path+folder+\"\\\\\"+file, cv2.IMREAD_COLOR)\n",
    "        bb = boxes[file]\n",
    "        max_side = max(bb[2], bb[3])\n",
    "\n",
    "        crop_img = img[bb[1]:bb[1]+bb[3], bb[0]:bb[0]+bb[2]]\n",
    "\n",
    "        if bb[1]+bb[3] > img.shape[0]:\n",
    "            bb[3] = crop_img.shape[0]\n",
    "        if bb[0]+bb[2] > img.shape[1]:\n",
    "            bb[2] = crop_img.shape[1]\n",
    "            \n",
    "        empty_x = math.ceil((max_side - bb[2])/2)\n",
    "        empty_y = math.ceil((max_side - bb[3])/2)\n",
    "        add_x = 0\n",
    "        add_y = 0\n",
    "        \n",
    "        if (max_side - bb[2])/2 != round((max_side - bb[2])/2):\n",
    "            add_x = 1\n",
    "        if (max_side - bb[3])/2 != round((max_side - bb[3])/2):\n",
    "            add_y = 1\n",
    "     \n",
    "        padding_img = np.zeros((max_side, max_side, 3), dtype=crop_img.dtype)\n",
    "\n",
    "        padding_img[empty_y:max_side-empty_y + add_y, empty_x:max_side-empty_x + add_x, :] = crop_img\n",
    "        if(max_side > bb[2]):\n",
    "            for i in range(empty_x):\n",
    "                tmp = np.random.normal(0, std_padding, padding_img[:, empty_x, :].shape)  \n",
    "                padding_img[:, i, :] = padding_img[:, empty_x, :] + tmp  \n",
    "            \n",
    "            for i in range(max_side-empty_x + add_x, padding_img.shape[1]):\n",
    "                padding_img[:, i, :] = padding_img[:, max_side-empty_x + add_x-1, :] + np.random.normal(0, std_padding, padding_img[:, max_side-empty_x + add_x-1, :].shape)\n",
    "        else:\n",
    "            for i in range(empty_y):\n",
    "                padding_img[i, :, :] = padding_img[empty_y, :, :] + np.random.normal(0, std_padding, padding_img[empty_y, :, :].shape)  \n",
    "            for i in range(max_side-empty_y + add_y, padding_img.shape[0]):\n",
    "                padding_img[i, :, :] = padding_img[max_side-empty_y + add_y-1, :, :] + np.random.normal(0, std_padding, padding_img[max_side-empty_y + add_y-1, :, :].shape)\n",
    "            \n",
    "        \n",
    "        padding_img[padding_img > 255] = 255\n",
    "        padding_img[padding_img < 0] = 0\n",
    "        \n",
    "        if index in test_elements:\n",
    "            if add_img:\n",
    "                continue\n",
    "                \n",
    "            resized_image = cv2.resize(padding_img, (size_img, size_img)) \n",
    "            \n",
    "            H0, hogImage0 = feature.hog(resized_image[:, :, 0], orientations=9, pixels_per_cell=(8, 8), cells_per_block=(3, 3), transform_sqrt=True, block_norm=\"L2-Hys\", visualize=True)\n",
    "            H1, hogImage1 = feature.hog(resized_image[:, :, 1], orientations=9, pixels_per_cell=(8, 8), cells_per_block=(3, 3), transform_sqrt=True, block_norm=\"L2-Hys\", visualize=True)\n",
    "            H2, hogImage2 = feature.hog(resized_image[:, :, 2], orientations=9, pixels_per_cell=(8, 8), cells_per_block=(3, 3), transform_sqrt=True, block_norm=\"L2-Hys\", visualize=True)\n",
    "            \n",
    "            hogImage = np.zeros(resized_image.shape)\n",
    "            hogImage[:, :, 0] = hogImage0\n",
    "            hogImage[:, :, 1] = hogImage1\n",
    "            hogImage[:, :, 2] = hogImage2  \n",
    "            hog = hogImage.reshape(hogImage.size)\n",
    "            test_set.append([hog, cls])\n",
    "        else:\n",
    "            for i in range(add_img_count + 1):\n",
    "                resized_image = cv2.resize(padding_img, (size_img, size_img)) \n",
    "\n",
    "                if i > 0 and add_type == 1:\n",
    "                    angle = (np.random.random()*max_angle*2)-max_angle                   \n",
    "                    resized_image = rotate(resized_image, angle, reshape=False, mode='nearest')\n",
    "                    if add_img_gauss:\n",
    "                        resized_image += np.random.normal(0, add_img_gauss,  resized_image.shape).astype(np.uint8)\n",
    "                if i > 0 and add_type == 2:\n",
    "                    shift_xy = (np.random.random(2)*max_shift*2)-max_shift\n",
    "                    resized_image = shift(resized_image, np.append(shift_xy, 0), mode='nearest')\n",
    "                    if add_img_gauss:\n",
    "                        resized_image += np.random.normal(0, add_img_gauss,  resized_image.shape).astype(np.uint8)\n",
    "                if i > 0 and add_type == 3:\n",
    "                    angle = (np.random.random()*max_angle*2)-max_angle                   \n",
    "                    shift_xy = (np.random.random(2)*max_shift*2)-max_shift\n",
    "                    resized_image = shift(resized_image, np.append(shift_xy, 0), mode='nearest')     \n",
    "                    resized_image = rotate(resized_image, angle, reshape=False, mode='nearest')\n",
    "                    if add_img_gauss:\n",
    "                        resized_image += np.random.normal(0, add_img_gauss, resized_image.shape).astype(np.uint8)\n",
    "\n",
    "                if np.random.random() < covered:\n",
    "                    x_start = np.random.randint(size_img)\n",
    "                    y_start = np.random.randint(size_img)\n",
    "                    resized_image[y_start:y_start+covered_size, x_start:x_start+covered_size, :] = 0\n",
    "\n",
    "                      \n",
    "                H0, hogImage0 = feature.hog(resized_image[:, :, 0], orientations=9, pixels_per_cell=(8, 8), cells_per_block=(3, 3), transform_sqrt=True, block_norm=\"L2-Hys\", visualize=True)\n",
    "                H1, hogImage1 = feature.hog(resized_image[:, :, 1], orientations=9, pixels_per_cell=(8, 8), cells_per_block=(3, 3), transform_sqrt=True, block_norm=\"L2-Hys\", visualize=True)\n",
    "                H2, hogImage2 = feature.hog(resized_image[:, :, 2], orientations=9, pixels_per_cell=(8, 8), cells_per_block=(3, 3), transform_sqrt=True, block_norm=\"L2-Hys\", visualize=True)\n",
    "\n",
    "                hogImage = np.zeros(resized_image.shape)\n",
    "                hogImage[:, :, 0] = hogImage0\n",
    "                hogImage[:, :, 1] = hogImage1\n",
    "                hogImage[:, :, 2] = hogImage2  \n",
    "\n",
    "                hog = hogImage.reshape(hogImage.size)\n",
    "\n",
    "# DO GENEROWANIA OBRAZKOW DO DOKUMETNACJI\n",
    "#                 fig, axes = plt.subplots(nrows=1, ncols=4)\n",
    "# #                 columns = ['Wejściowe', 'Odcięte', 'Normalizacja', 'HOG']\n",
    "# #                 images = [img, crop_img, resized_image, hogImage]\n",
    "            \n",
    "#                 images = [img, crop_img, resized_image, hogImage]\n",
    "# #                 columns = ['Wejściowe', 'Odcięte', 'Eksperyment', 'HOG']\n",
    "#                 columns = ['Wejściowe', 'Odcięte', 'Zakryte', 'HOG']\n",
    "\n",
    "#                 for col in range(len(columns)):\n",
    "#                         axes[col].axis('off')\n",
    "#                         axes[col].imshow(images[col])\n",
    "\n",
    "#                 for ax, col in zip(axes, columns):\n",
    "#                     ax.set_title(col)\n",
    "\n",
    "#                 fig.tight_layout()\n",
    "\n",
    "#                 plt.savefig('fig/'+str(int(k))+\"_\"+str(i)+'.jpg', dpi = 300,bbox_inches='tight')\n",
    "#                 plt.show()\n",
    "\n",
    "                training_set.append([hog, cls])\n",
    "\n",
    "                if not add_img:\n",
    "                    break;\n",
    "test_set = np.array(test_set)\n",
    "training_set = np.array(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zapis danych!\n",
    "\n",
    "# test_set_file = open('128_128_test.data', 'wb')\n",
    "# pickle.dump(test_set, test_set_file, protocol= 4) \n",
    "# test_set_file.close()\n",
    "\n",
    "# train_set_file = open('128_128_train.data', 'wb')\n",
    "# pickle.dump(training_set, train_set_file, protocol= 4)\n",
    "# train_set_file.close()\n",
    "\n",
    "# train_set_file = open('128_128_train_rotate_5.data', 'wb')\n",
    "# pickle.dump(training_set, train_set_file, protocol= 4)\n",
    "# train_set_file.close()\n",
    "\n",
    "# train_set_file = open('128_128_train_shift_5.data', 'wb')\n",
    "# pickle.dump(training_set, train_set_file, protocol= 4)\n",
    "# train_set_file.close()\n",
    "\n",
    "# train_set_file = open('128_128_train_rotate_shift_5.data', 'wb')\n",
    "# pickle.dump(training_set, train_set_file, protocol= 4)\n",
    "# train_set_file.close()\n",
    "\n",
    "# train_set_file = open('128_128_train_rotate_shift_gauss_5.data', 'wb')\n",
    "# pickle.dump(training_set, train_set_file, protocol= 4)\n",
    "# train_set_file.close()\n",
    "\n",
    "train_set_file = open('128_128_train_rotate_shift_gauss_covered_15_5.data', 'wb')\n",
    "pickle.dump(training_set, train_set_file, protocol= 4)\n",
    "train_set_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Odczyt danych!\n",
    "\n",
    "test_set_file = open('128_128_test.data', 'rb')\n",
    "test_set = pickle.load(test_set_file)\n",
    "test_set_file.close()\n",
    "\n",
    "train_set_file = open('128_128_train.data', 'rb')\n",
    "training_set = pickle.load(train_set_file)\n",
    "train_set_file.close()\n",
    "\n",
    "# train_set_file = open('128_128_train_rotate_5.data', 'rb')\n",
    "# training_set = pickle.load(train_set_file)\n",
    "# train_set_file.close()\n",
    "\n",
    "# train_set_file = open('128_128_train_shift_5.data', 'rb')\n",
    "# training_set = pickle.load(train_set_file)\n",
    "# train_set_file.close()\n",
    "\n",
    "# train_set_file = open('128_128_train_rotate_shift_5.data', 'rb')\n",
    "# training_set = pickle.load(train_set_file)\n",
    "# train_set_file.close()\n",
    "\n",
    "# train_set_file = open('128_128_train_shift_gauss_5.data', 'rb')\n",
    "# training_set = pickle.load(train_set_file)\n",
    "# train_set_file.close()\n",
    "\n",
    "# train_set_file = open('128_128_train_rotate_shift_gauss_5.data', 'rb')\n",
    "# training_set = pickle.load(train_set_file)\n",
    "# train_set_file.close()\n",
    "\n",
    "# train_set_file = open('128_128_train_rotate_shift_gauss_covered_15_5.data', 'rb')\n",
    "# training_set = pickle.load(train_set_file)\n",
    "# train_set_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_hist_1 = []\n",
    "perceptron_hist_2 = []\n",
    "perceptron_cmc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_size_base = 32\n",
    "# layers = np.array([2, 4, 5, 8, 10])\n",
    "# layers = np.array([3, 6, 9, 12])\n",
    "# layers = np.array([5, 10, 15])\n",
    "# layers = np.array([10, 20])\n",
    "layers = np.array([30])\n",
    "\n",
    "layers = layers * layer_size_base\n",
    "\n",
    "input_size = training_set[0][0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perceptron is not None:\n",
    "    del perceptron\n",
    "    perceptron = None \n",
    "perceptron = Sequential()\n",
    "perceptron.add(Dense(layers[0], input_dim=input_size))\n",
    "perceptron.add(Activation('relu'))\n",
    "for l in layers[1:len(layers)]:\n",
    "    perceptron.add(Dense(l))\n",
    "    perceptron.add(Activation('relu'))\n",
    "perceptron.add(Dense(class_count))\n",
    "perceptron.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "perceptron.compile(optimizer=SGD(lr=0.005, momentum=0.9),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', top_k(5)])\n",
    "perceptron_hist_1.append(perceptron.fit(np.stack(training_set[:, 0], axis = 0), \n",
    "          np_utils.to_categorical(training_set[:, 1], num_classes=class_count), \n",
    "          epochs=15, \n",
    "          batch_size=32,\n",
    "          verbose = 1))\n",
    "\n",
    "# perceptron.compile(optimizer=SGD(lr=0.0005, momentum=0.9),\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy', top_k(5)])\n",
    "# perceptron_hist_2.append(perceptron.fit(np.stack(training_set[:, 0], axis = 0), \n",
    "#           np_utils.to_categorical(training_set[:, 1], num_classes=class_count), \n",
    "#           epochs=25, \n",
    "#           batch_size=32,\n",
    "#           verbose = 1))\n",
    "\n",
    "perceptron_cmc.append(cmc(perceptron, test_set, False, None))\n",
    "history(perceptron_hist_1[-1], perceptron_hist_2[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# more_cmc(perceptron_cmc, [5, 4, 3, 2, 1], 'Liczba warstw', 'perceptron_cmc.png')\n",
    "# more_history(perceptron_hist_1, perceptron_hist_2, 'acc', [5, 4, 3, 2, 1], 'Liczba warstw', 'perceptron_history_1.png')\n",
    "# more_history(perceptron_hist_1, perceptron_hist_2, 'top_5', [5, 4, 3, 2, 1], 'Liczba warstw', 'perceptron_history_2.png')\n",
    "# np.savetxt('perceptron_cmc.txt', np.array(perceptron_cmc), fmt = '%0.2f')\n",
    "# for i, j in zip(perceptron_hist_1, perceptron_hist_2):\n",
    "#     history(i,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sieć splotowa - VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cnn_ref is not None:\n",
    "    del cnn_ref\n",
    "    del base_cnn_ref\n",
    "    cnn_ref = None \n",
    "    base_cnn_ref = None\n",
    "\n",
    "base_cnn_ref = VGG19(weights='imagenet', input_shape = (size_img, size_img, 3), include_top=False)\n",
    "\n",
    "x = base_cnn_ref.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "predictions = Dense(class_count, activation='softmax')(x)\n",
    "cnn_ref = Model(inputs=base_cnn_ref.input, outputs=predictions)\n",
    "\n",
    "for layer in base_cnn_ref.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in base_cnn_ref.layers[12:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cnn_ref.compile(optimizer=SGD(lr=0.005, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy', top_k(5)])\n",
    "cnn_ref_hist_1 = cnn_ref.fit(np.stack(training_set[:, 0], axis = 0).reshape((training_set.shape[0], size_img, size_img, 3)), \n",
    "          np_utils.to_categorical(training_set[:, 1], num_classes=class_count), \n",
    "          epochs=15, \n",
    "          batch_size=32,\n",
    "          verbose = 1)\n",
    "\n",
    "cnn_ref.compile(optimizer=SGD(lr=0.0005, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy', top_k(5)])\n",
    "cnn_ref_hist_2 = cnn_ref.fit(np.stack(training_set[:, 0], axis = 0).reshape((training_set.shape[0], size_img, size_img, 3)), \n",
    "          np_utils.to_categorical(training_set[:, 1], num_classes=class_count), \n",
    "          epochs=20, \n",
    "          batch_size=32,\n",
    "          verbose = 1)\n",
    "\n",
    "cnn_ref_cmc = cmc(cnn_ref, test_set, True, None)\n",
    "history(cnn_ref_hist_1, cnn_ref_hist_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_cmc([cnn_ref_cmc], ['VGG19'], 'Nazwa sieci', 'vgg19_cmc.png')\n",
    "more_history([cnn_ref_hist_1], [cnn_ref_hist_2], 'acc', ['VGG19'], 'Nazwa sieci', 'vgg19_history_1.png')\n",
    "more_history([cnn_ref_hist_1], [cnn_ref_hist_2], 'top_5', ['VGG19'], 'Nazwa sieci', 'vgg19_history_2.png')\n",
    "with open('vgg19_cmc.txt', 'a') as file:\n",
    "    np.savetxt(file, np.array(cnn_ref_cmc), fmt = '%0.5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sieć splotowa - nasza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_hist_1 = []\n",
    "cnn_hist_2 = []\n",
    "cnn_cmc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('cnn_cmc.txt', 'r') as file:\n",
    "#     cnn_cmc = np.loadtxt(file)\n",
    "# # cnn_cmc = cnn_cmc.tolist()\n",
    "# cnn_cmc = [cnn_cmc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Sequential()\n",
    "cnn.add(Conv2D(1, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=(size_img, size_img, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 124, 124, 1)       76        \n",
      "=================================================================\n",
      "Total params: 76\n",
      "Trainable params: 76\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = True\n",
    "droput_rate = 0.5\n",
    "\n",
    "descriptor_size = 1024\n",
    "\n",
    "if cnn is not None:\n",
    "    del cnn\n",
    "    cnn = None \n",
    "\n",
    "cnn = Sequential()\n",
    "cnn.add(Conv2D(128, kernel_size=(3, 3), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=(size_img, size_img, 3)))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "cnn.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "cnn.add(Flatten())\n",
    "\n",
    "cnn.add(Dense(descriptor_size, activation='relu'))\n",
    "\n",
    "if dropout:\n",
    "    cnn.add(Dropout(droput_rate))\n",
    "    \n",
    "cnn.add(Dense(class_count, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer=SGD(lr=0.005, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy', top_k(5)])\n",
    "cnn_hist_1.append(cnn.fit(np.stack(training_set[:, 0], axis = 0).reshape((training_set.shape[0], size_img, size_img, 3)), \n",
    "          np_utils.to_categorical(training_set[:, 1], num_classes=class_count), \n",
    "          epochs=7, \n",
    "          batch_size=32,\n",
    "          verbose = 1))\n",
    "\n",
    "cnn.compile(optimizer=SGD(lr=0.0005, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy', top_k(5)])\n",
    "cnn_hist_2.append(cnn.fit(np.stack(training_set[:, 0], axis = 0).reshape((training_set.shape[0], size_img, size_img, 3)), \n",
    "          np_utils.to_categorical(training_set[:, 1], num_classes=class_count), \n",
    "          epochs=5, \n",
    "          batch_size=32,\n",
    "          verbose = 1))\n",
    "\n",
    "cnn_cmc.append(cmc(cnn, test_set, True, None))\n",
    "history(cnn_hist_1[-1], cnn_hist_2[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "title = 'Pokrycie'\n",
    "# labels = ['Podstawowy', 'Obrót', 'Translacja', 'Translacja + Obrót', 'Translacja + Obrót + Szum']\n",
    "# labels = ['Podstawowy (512)', '256', '1024']\n",
    "labels = ['Nie', 'Tak (15%)']\n",
    "\n",
    "more_cmc(cnn_cmc, labels, title, 'cnn_cmc_pokrycie.png')\n",
    "more_history(cnn_hist_1, cnn_hist_2, 'acc', labels[1:], title, 'cnn_pokrycie_history_1.png')\n",
    "more_history(cnn_hist_1, cnn_hist_2, 'top_5', labels[1:], title, 'cnn_pokrycie_history_2.png')\n",
    "\n",
    "with open('cnn_cmc_pokrycie.txt', 'w') as file:\n",
    "    np.savetxt(file, np.array(cnn_cmc), fmt = '%0.5f')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn.save('cnn_gauus_shift_rotate_5_1024_dropout_covered_15.h5')\n",
    "# cnn = load_model('cnn_gauus_shift_rotate_5_1024_dropout_covered_15.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer = -3 #Flatten\n",
    "layer = -2 #Dense\n",
    "if dropout:\n",
    "    layer -= 1\n",
    "    \n",
    "svm_cnn = Model(inputs=cnn.input, outputs=cnn.layers[layer].output)\n",
    "svm_descriptors = svm_cnn.predict(\n",
    "    np.stack(training_set[:, 0], axis = 0).reshape((training_set.shape[0], size_img, size_img, 3)), \n",
    "    batch_size=32,\n",
    "    verbose = 1)\n",
    "datetime.datetime.now()\n",
    "svm_clf = svm.SVC(decision_function_shape='ovr', probability = True, verbose = False)\n",
    "svm_clf.fit(svm_descriptors, training_set[:, 1].astype(np.int32))\n",
    "datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "svm_test_descriptors = svm_cnn.predict(\n",
    "    np.stack(test_set[:, 0], axis = 0).reshape((test_set.shape[0], size_img, size_img, 3)), \n",
    "    batch_size=32,\n",
    "    verbose = 1)\n",
    "\n",
    "svm_results = svm_clf.predict_proba(svm_test_descriptors)\n",
    "cnn_cmc.append(cmc(None, test_set, False, svm_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Metoda klasyfikacji'\n",
    "labels = ['CNN', 'SVM']\n",
    "\n",
    "more_cmc(cnn_cmc, labels, title, 'cnn_cmc_svm.png')\n",
    "\n",
    "with open('cnn_cmc_svm.txt', 'w') as file:\n",
    "    np.savetxt(file, np.array(cnn_cmc), fmt = '%0.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('svm.model', 'wb') as f:\n",
    "#     pickle.dump(svm_clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cnc_summary.txt', 'r') as file:\n",
    "    summary = np.loadtxt(file)\n",
    "summary = summary.tolist()\n",
    "title = \"Podsumowanie\"\n",
    "labels = [\"VGG19\", \"CNN - Zakrycie (15% obrazów)\", \"SVM\", \"CNN - dropout (50%)\", \"CNN - Rotacja + translacja\", \"Perceptron - 1 warstwa\"]\n",
    "more_cmc(summary, labels, title, 'summary.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
